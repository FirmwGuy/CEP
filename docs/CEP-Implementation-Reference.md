# CEP Implementation Reference

## Introduction
This document condenses every implementation-relevant rule and guarantee scattered across `docs/`, mirroring the refreshed storyline in `docs/CEP.md`. It keeps the focus on the shipped Layer 0 kernel while summarizing how the planned packs (Layers 1–4) constrain today’s interfaces, so automated agents can crosswalk the narrative beats to deterministic contracts without rereading every design paper.

## Technical Details

### Layer 0 – Implemented surface (shipping)
- Kernel heartbeat and storage: deterministic Capture → Compute → Commit, cells/stores/CAS/CPS, append-only history, locks, link/shadow bookkeeping, and async I/O with flat serializer frames.
- Security and federation: enclave policy loader/enforcement, gateway validation, pipeline preflight hook, CEI surfaces (`sec.edge.deny`, `sec.limit.hit`, `sec.pipeline.reject`), and transport manager plumbing with capability negotiation.
- Pipeline metadata plumbing: `pipeline_id`/`stage_id` (plus optional DAG run/hop) carried through envelopes, watchers, CEI origin, async requests, federation invoke/link/mirror, and enzyme contexts without interpreting graphs.
- Episodic engine and OPS timelines: `op/boot`/`op/shdn`, `op/ep`, watcher semantics, pause/rollback/resume, and diagnostics mailboxes.
- Persistence: CPS controllers, flatfile backend, async commit flow, branch policies (`allow_volatile_reads`, `snapshot_ro`), Decision Cells for cross-branch reads, and CEI telemetry for commits/flushes/checkpoints.
- Tooling/testing stance: ASAN/Valgrind separation, sanitizer build directory split, code map generation, and opt-in pack testing conventions.

### Layer 1 – Current implementation (optional pack)
- Pack bootstrap/shutdown helpers create `/data/coh/**` and `/data/flow/**`, register the adjacency-closure enzyme, and publish pack readiness via `op/l1_boot`/`op/l1_shdn`. `op/coh_sweep` rebuilds adjacency and debts on demand.
- Coherence scaffolding: helpers add beings, bonds, contexts, facets, and debts with namepool-backed IDs; contexts accept role bindings, closure validates roles/facets against `/data/coh/schema/ctx_rules/**`, materializes facets, records debts (`coh.debt.new`/`coh.debt.resolved`/`coh.role.invalid`/`coh.closure.fail`), and keeps append-only debt history with `ctx_kind` lineage. Adjacency mirrors live under `/data/coh/adj/**`.
- Pipeline DAG scaffolding: ensures pipeline/stage/edge dictionaries under `/data/flow/pipelines/**`, records IDs/revisions/owners/provinces, enforces basic edge sanity (no self-loops, `max_hops` ceiling), and binds pipelines/stages back to coherence beings via `has_stage` bonds and `pipeline_edge` contexts.
- Runtime scaffolding: records runs under `/data/flow/runtime/runs/**`, mirrors pipeline stages/edges into runs to seed `fan_in`, tracks per-stage state/triggers/metrics/annotations, accumulates pipeline + stage metrics, emits CEI `flow.dispatch.blocked` when pause/rollback gates dispatch, emits `flow.pipeline.missing_metadata` when metadata is absent, and auto fan-outs along recorded edges bumping hop indexes.
- Federation alignment: invoke helpers require pipeline metadata, attach it to requests, and emit `sec.pipeline.reject` or `flow.pipeline.missing_metadata` on failures; link/mirror mount helpers stamp pipeline metadata and fail fast if IDs cannot be interned.
- Hydration helper: `cep_l1_coh_hydrate_safe` wraps the L0 hydrate call with optional snapshot view and CEI on failures (`coh.hydrate.fail`, `coh.cross_read` for allowed cross-branch reads).
- Testing: opt-in smoke suite under `src/test/l1_coherence/` gated by `CEP_L1_TESTS`, covering closure/debts, pipeline provenance, runtime fan-in/out, and federation metadata preparation.

### Layer scope and adoption order
- CEP’s layers align with `docs/CEP.md`: **Layer 0 – Kernel & Pipeline Substrate**, **Layer 1 – Coherence & Pipeline Graphs**, **Layer 2 – Ecology & Flows (Modules and Evolution)**, **Layer 3 – Awareness, Datasets & Human Interaction**, and **Layer 4 – Governance, Safety & Self‑Evolution**. Only Layer 0 ships in this repository; higher layers remain planned but their invariants (Decision Cells, facet closure, supervisory approvals) inform the kernel contracts and API surface.
- Beats remain strictly Capture → Compute → Commit. Inputs for beat *N* freeze during Capture, work executes during Compute, and results become visible at beat *N + 1*. Every non-deterministic choice must emit a Decision Cell so replays consume recorded decisions instead of re-executing randomness, matching Layer 0’s heartbeat description in `docs/CEP.md §2.1`.
- Minimal adoption sequence mirrors `docs/CEP.md §2.9`: start with the kernel/heartbeat, add coherence (Beings/Bonds/Contexts) when identities matter, then introduce the Flow VM (Guard/Transform/Wait/Decide/Clamp primitives plus Species/Variants/Niches/Guardians), layer in perspectives/interpretations for awareness, and finally add governance packs (Laws/Reforms/Councils/Provinces) so reforms and upgrades stay auditable.
- Layer 1 ensures adjacency closure: contexts tie multiple beings with role-typed positions and automatically materialize required facets or record debts; adjacency mirrors accelerate local queries while `/data/coh/*` stays authoritative.
- Observability remains integral: provenance links every derived fact to sources, guard predicates, code identities, and council approvals. Replay can re-run any beat range with Decision Cells to ensure equality, and summarization layers (Perspectives, Interpretations, Conventions, Summaries) build incremental awareness artifacts.
- Security telemetry lands in three canonical surfaces:
  - `/sys/state/security` publishes the live readiness record with `state`, `note`, `fault`, policy version (`pol_ver`), and the beat that wrote it. Reloads switch to `state=loading` before emitting `state=ready`, and parse/enforcement failures store the fault text so bootstrap can halt deterministically.
  - `/rt/analytics/security` aggregates enforcement results: `edges/<from>/<to>` and `gateways/<gateway>` maintain cumulative allow/deny counters, while `beats/<bt>` records the per-beat digest (allow, deny, and limit-hit tallies) for quick auditing without replaying Decision Cells.
  - CEI topics are fixed: `sec.edge.deny`, `sec.branch.deny`, `sec.limit.hit`, and `sec.pipeline.reject` cover runtime edges, branch guards, budget ceilings, and pipeline preflight failures respectively. These emissions now carry a `pipeline` sub-dictionary (when metadata is available) containing `pipeline_id`, `stage_id`, `dag_run_id`, and `hop_index` so limit hits and preflight rejects remain attributable to the originating pipeline stage.
- Enclave policy enforcement is part of the shipping kernel:
  - `cep_enclave_policy` snapshots `/sys/security/{enclaves,edges,gateways,branches,defaults,env/**}` at bootstrap, maintains watchers, and records readiness/fault metadata in `/sys/state/security`. Policy reloads are deterministic: `state=loading` during parse, `state=ready pol_ver=<hash>` when the new snapshot succeeds, `fault=<reason>` otherwise.
  - Gateway validators (`cep_fed_invoke_validator()`, `cep_fed_transport_manager_send*()`) consult the resolved edge + context tuple before any cross‑enclave mutation. The resolver emits the most restrictive ceilings (budgets, TTL, per-edge QPS), and runtime counters decrement each beat while emitting CEI events when a ceiling trips.
  - Sensitive internal branches reuse the same policy primitives via `cep_cell_svo_context_guard()`, so crown‑jewel paths inside the trusted enclave remain behind explicit allow rules even though everything else runs open.
  - Pipelines must be approved before execution. The `sig_sec/pipeline_preflight` enzyme validates `/data/<pack>/policy/security/pipelines/*` graphs against the current snapshot, stamps `state`/`note`/`pol_ver`/`beat`, and `cep_fed_invoke_validator()` refuses cross‑enclave requests that lack a valid pipeline ID.
  - Operator workflow: edit the policy tree, wait for `/sys/state/security` to return `state=ready`, re-run `sig_sec/pipeline_preflight` if pipelines drift, watch `/rt/analytics/security/*` for allow/deny counters, and subscribe to the `sec.*` CEI topics for per-event details. `CEP_ENABLE_DEBUG=1` enables `[svo_guard]` breadcrumbs inside `cep_cell_svo_context_guard()`, while `TEST_BRANCH_DEBUG=1` dumps the diagnostics mailbox when `/CEP/branch/security_guard` fails so branch denials always show their `topic`/`note`.
  - Tests and tooling: follow the Enclave validation workflow in `docs/BUILD.md` to rerun the targeted suites, full default/ASAN sweeps, lexicon checks, and Valgrind batches; this keeps `docs/CEP-TAG-LEXICON.md`, `/sys/state/security`, and `/rt/analytics/security` aligned with the shipping resolver.
- Privacy relies on payload-level cryptography with per-subject keys; erasure drops keys while keeping structural stubs, and redaction cells can provide reversible masking. Scale is horizontal via partitions exchanging serialized deltas rather than global barriers; occasional sync pulses align summaries.

### Observability, privacy, and replay (per `docs/CEP.md §2.7`)
- Provenance chains in Layer 0 link every derived cell back to sources, guard predicates, code identities, and Decision Cells. OPS dossiers under `/rt/ops/**` track kernel boot/shutdown, persistence, async I/O, and pack-defined jobs so Layer 3 can materialize perspectives without bespoke plumbing.
- CEI captures topic/severity/note/origin plus attachments; mailboxes inherit TTL and retention rules regardless of which pack owns them, keeping governance stories explainable.
- Privacy hooks (payload-level cryptography, secmeta fingerprints, reversible redaction cells) live in `cepData`. Erasure drops keys yet leaves stubs so higher layers can prove history without revealing payloads.
- Replay consumes Decision Cells, CPS logs, and serialized frames. Re-running any beat range with side effects disabled yields byte-for-byte parity, enabling deterministic investigations across the beats described in `docs/CEP.md`.

### Scale and federation alignment (`docs/CEP.md §2.8`)
- CEP scales horizontally by partitioning `/data/**` into branches with their own heartbeat and CPS configuration; determinism plus adjacency mirrors keep local work self-contained rather than imposing global barriers.
- Federation is transport-neutral: providers register caps (CRC32C/deflate/AEAD/comparator ceilings, unreliable for `upd_latest`) and the transport manager mirrors them under `/net/transports/**`. Mounts live under `/net/mounts/<peer>/<mode>/<mount>/` with required/preferred caps, `upd_latest`, chosen provider, and negotiated serializer policy. Discovery/health organs keep `/net/peers/**` and `/net/telemetry/**` clean, and link/mirror/invoke validators emit CEI (`tp_noprov`, `tp_flatneg`, `tp_inv_timeout`, etc.) plus CEH mirrors under `/net/peers/<peer>/ceh/*`. All traffic is flat-serializer frames; serializer downgrades are recorded, and `upd_latest` only works on unreliable transports.
- Invoke requests carry `pipeline` metadata and are checked against enclave/pipeline approvals and budgets; denials emit `sec.pipeline.reject`/`sec.edge.deny`/`sec.limit.hit` with `origin/pipeline` populated. Link/mirror honour negotiated serializer policy and backpressure semantics.
- Governance packs coordinate multi-province reforms through the same telemetry surfaces instead of bespoke channels.

### Pipeline metadata and hydration (L0 API)
- L0 is graph-agnostic but carries a pipeline block everywhere: envelopes/watchers/CEI/async and enzyme contexts expose `pipeline_id`, `stage_id`, optional `dag_run_id` and `hop_index`. Federation invoke encodes the same block; validator rejects cross-enclave calls lacking approved `pipeline_id`/`stage_id`.
- `pipeline` dictionaries live under `/rt/ops/<oid>/envelope`, watcher entries, CEI `origin/pipeline`, and federation requests. Enzymes read the IDs from `ctx->pipeline` to tag CEI or runtime cells.
- Hydrating off-RAM cells inside pipeline-aware enzymes uses `cep_cell_hydrate_for_enzyme(ref, ctx, opts, result)`: options cover snapshot vs live view, cross-branch allow/deny, Decision Cell requirement, depth/byte budgets, ancestor locks, and child/payload prefetch. Policy guardrails (`cep_branch_policy_check_read`) enforce `allow_volatile_reads`/snapshot rules; risky cross-branch reads record Decision Cells for replay. Failures (policy, budgets, CAS gaps) never publish partial state.

### Minimal viable stack (recap of `docs/CEP.md §2.9`)
1. **Layer 0 – Kernel & Pipeline Substrate:** deterministic heartbeat, cells/stores, CPS/CAS, federation, security, pipeline metadata plumbing.
2. **Layer 1 – Coherence & Pipeline Graphs:** beings, bonds, contexts, facets, adjacency mirrors, pipeline graphs.
3. **Layer 2 – Ecology & Flows:** flows/modules/species/variants/niches/guardians with Decision Cells and supervised parameter updates.
4. **Layer 3 – Awareness & Human Interaction:** perspectives, interpretations, conventions, summaries, labeling/override surfaces.
5. **Layer 4 – Governance & Self‑Evolution:** laws, reforms, councils, provinces, and upgrade pipelines that record every decision.

Each layer adds structure or oversight while preserving the determinism/replay guarantees the rest of this document details.

### Worked example (aligned with `docs/CEP.md §2.10`)
- `Event#view1` lands in Layer 0 under `/data/app/events/**`, carrying `pipeline_id=learn/feed_ranking` and the stage that logged it.
- Layer 1 ties beings (`user:alice`, `model:feed_ranking`, `dataset:home_feed`) via contexts/facets, routes the `FeedRanking` pipeline stages, and pushes adjacency indexes for later lookups.
- Layer 2’s flow `TrainFeed` emits Decision Cells for variant picks, records predictions/training examples, and stages parameter updates under `/data/learn/models/**`.
- Layer 3 surfaces variant performance, cohort health, and label quality so humans can add overrides/labels that flow back as cells and CEI facts.
- Layer 4 encodes reforms to promote `feed_ranking:v3`, complete with rollout/rollback triggers and metrics; upgrade pipelines run as CEP episodes so every approval is replayable.

### Domain/Tag naming, lexicon, and glob rules
- `cepDT` stores 58-bit domain and tag identifiers. Layer 0 defaults to the `CEP` domain; tags are lowercase “words” up to 11 characters drawn from `[a-z0-9:_-.]`. Acronyms allow up to 9 printable ASCII characters (0x20–0x5F). `*` marks globbing segments and is only valid when the runtime sets the glob bit.
- `CEP-TAG-LEXICON.md` enumerates canonical tags for runtime roots, heartbeat artifacts, transports, and harness fixtures. Extend it before minting new identifiers so tooling (lexicon checkers) can spot unused tags.
- Glob semantics: a globbed tag matches zero or more characters within the segment; domains still use dedicated sentinel IDs. Literal cell names must not contain glob characters; globbing is reserved for queries (enzyme descriptors, traversal filters). Serialization persists the glob bit so replay does not recompute intent.
- The namepool interns longer or user-provided identifiers; reference IDs are append-only and replayed deterministically. Use `cep_namepool_intern_pattern*` when the stored string should behave as a glob. Release dynamic entries when modules unload to prevent pool growth.

### Data model, stores, history, and traversal
- Cells carry metadata, optional payload (`cepData`), and optional child stores (`cepStore`). Append-only timelines apply independently to payloads and stores. Each mutation records a `modified` beat, and past entries chain backwards to keep history O(1) per write. Duplicate payloads or structural no-ops short-circuit updates.
- Stores are pluggable (linked list, array, packed queue, red-black tree, hash table, octree) and pair with indexing modes (insertion, by name, by comparator, by hash, by function). Choose storage and indexing per workload; avoid unnecessary reindexing because each snapshot captures a full layout history.
- Locks prevent concurrent mutations: `cep_store_lock`/`unlock` gates structure, `cep_data_lock`/`unlock` gates payloads, and locks propagate up the ancestor chain. Immutable branches (`cep_cell_set_immutable`, `cep_branch_seal_immutable`) reject mutations outright.
- Links resolve to non-link targets and maintain backlink “shadows” so targets cannot finalize while referenced. Tombstone propagation marks linkers when targets are deleted. Link cycles remain disallowed.
- Raw traversal helpers (`cep_cell_*_all`) expose veiled and deleted children without visibility filters, enabling sealing/digesting routines to inspect every stored child.
- `cep_cell_add` insists on an active RW lease (`cep_ep_require_rw()`); rejected attempts immediately finalize floating children so callers do not leak scratch nodes, and successful inserts copy the parent store’s `modified` beat into the child’s `created` timestamp when the parent is already published. Treat add failures as destructive for the supplied scratch cell.
- Secured payloads: `cepData` embeds a `secmeta` snapshot (fingerprint, raw/encrypted lengths, codec, AEAD mode, key identifier, nonce, AAD hash). The public setters (`cep_data_set_plain/cdef/enc/cenc`) record those fields, encrypt/compress in-place, and expose the exact bytes the serializer/CPS will persist. `cep_data_unveil_ro()` produces short-lived plaintext views and `cep_data_unveil_done()` zeroes/frees them so plaintext never survives outside the caller. Rekey/recompress helpers update the same `secmeta` data, stamp the flags `ram_enc` and `ram_cas` under `meta/sec/` when the in-RAM payload is sealed and backed by CAS, and emit CEI (`enc_fail`, `dec_fail`, `rekey_fail`, `codec_mis`) on failure. When long-running packs want deterministic verbs they must stick to the reserved OPS tags `rekey` (`op/rekey`) and `rcomp` (`op/rcomp`) so telemetry remains searchable.
- Provenance buckets live under `meta/parents`: callers toggle the `meta` dictionary/list stores writable, hard-delete previous entries, and append link cells tagged with `parent` to capture each source. The helper enforces normal-cell targets and refuses immutable nodes.
- `cep_cell_initialize_clone` only mirrors metadata for empty template shells: the source must lack payloads and child stores, so use the helper strictly to seed shape-identical containers before running specialised clone routines.
- Root layout is fixed: `/sys` (lifecycle state), `/rt` (beat journals, ops, analytics), `/journal` (effect logs), `/env` (handles), `/cas` (content-addressable blobs), `/lib` (library snapshots), `/data` (durable state promoted at commit), `/tmp` (scratch), `/enzymes` (registry manifests). Heartbeat policy optionally creates `/rt/beat/<n>/` directories containing `impulses`, `meta/unix_ts_ns`, `agenda`, and `stage`.

### Payload representations and external resources
- Payload kinds: `VALUE` (inline bytes, best for ≤64 B), `DATA` (heap buffer, optional destructor, `swap=true` for zero-copy), `HANDLE` (opaque reference managed by adapters), `STREAM` (windowed I/O). Only `VALUE`/`DATA` expose direct buffers; handles and streams must use adapter APIs.
- Native types treat all bytes as opaque; canonicalization (endianness, UTF‑8, vector layouts) lives in enzymes. Hashes combine domain, tag, size, and bytes for deterministic indexing.
- Proxies wrap external resources. Adapters implement retain/release, snapshot/restore, and stream read/write/map hooks. Flat serialization emits proxy envelopes inside `payload_chunk` records (the datatype byte distinguishes HANDLE vs. STREAM), and adapters refusing snapshots must abort serialization explicitly. Deep clones of HANDLE/STREAM nodes become links to avoid duplicating resources.
- External library access is mediated by adapter vtables. CEP never touches foreign structs directly; deterministic work requires either a snapshot (bytes stored in a cell) or a read-only borrowed view that obeys strict lifetime guarantees. Mutations against external systems must supply preconditions (hash, version, ETag) and record intent/outcome pairs in the journal.
- I/O effect logging: every read/write captures offset, length, hash, expected version, idempotency key, and result. Large transfers rely on windowing plus optional checkpoints; replays can simulate (no side effects) or re-apply (verify preconditions before touching the world).
- Library bindings are DATA payloads typed as `library` cells; they own refcounted adapters whose destructor marks the binding “destroying” before releasing the context. HANDLE/STREAM reads or writes append `stream-log` VALUE entries under a lazily created `journal` list child on the owning resource, capturing offsets, requested/actual byte counts, hashes, flags, and a beat-derived Unix timestamp so traces stay deterministic.

### Deterministic runtime, enzymes, and episodes
- Heartbeat plumbing (`cep_heartbeat_*`) enforces beat phases, records impulses under `/rt/beat/<n>/impulses`, logs agenda execution, and writes commit notes. Impulses log both signal and target paths. Beat timestamps land in `/rt/beat/<n>/meta/unix_ts_ns` and analytics capture spacing deltas.
- Enzyme descriptors (`cep_enzyme_register`) define callbacks, match policy (exact or prefix), dependency lists, labels, and idempotency flags. Cell bindings (`cep_cell_bind_enzyme`) attach descriptors to subtrees with optional propagation; tombstones mask inherited bindings. Signal-only dispatch reuses registry indexes with literal buckets plus wildcard head lists.
- Dispatch resolves bindings along the target ancestry, intersects with signal matches, performs specificity and dependency ordering, and topologically sorts the agenda. Pending registrations activate on the next beat to keep agendas stable.
- Cell-operation enzymes (`sig_cell/op_*`) provide stock add/update/delete/move/clone verbs operating on request dictionaries containing role links (`role_parent`, `role_subject`, `role_source`, `role_templ`) plus optional arguments (`arg_deep`, `arg_prepend`, `arg_pos`).
- The Episodic Enzyme Engine (E3) tracks long-running work as `op/ep` dossiers. Episodes run slices under policies (RO, RW, hybrid), use TLS contexts for budgets and cancellation, and require leases for mutations. Yield (`cep_ep_yield`) and await (`cep_ep_await`) state transitions are recorded in OPS history. RW promotions acquire leases before mutating; demotions enforce lease release. Budget overruns emit `ep:bud/*` CEI facts and flip cancellation flags.
- Pause/Rollback/Resume controls gate heartbeat agendas: pause acquires locks and parks non-control impulses in `/data/mailbox/impulses` (with QoS flags `retain_on_pause` / `discard_on_rollback`), rollback rewinds the published `view_hzn` and drops discard-marked messages, and resume drains the backlog in deterministic ID order before clearing `/sys/state/paused` and `/sys/state/view_hzn`. Only control impulses and CEI continue while gating is active; failures close the control OPS dossier with `sts:fail`.
- Debug macros: wrap instrumentation in `CEP_DEBUG`, guard invariants with `CEP_ASSERT`/`CEP_NOT_ASSERT`, and avoid placing required logic inside debug-only blocks so release builds stay correct.

### Async I/O fabric and OPS observability
- `cep_async_ops_oid()` lazily opens the `op/io` dossier (verb `op/io`, target `/rt/async`, mode `opm:states`) and the public helpers (`cep_async_register_channel`, `cep_async_register_request`, `cep_async_post_completion`) wrap the lower-level `cep_op_async_record_*` APIs so every async submission is mirrored under `/rt/ops/<oid>/`. Channels (`io_chan/<channel>/`) capture `target_path`, `provider`, `reactor`, `caps`, and a `shim` flag plus a `watchers/` dictionary so awaiters can subscribe to CTS/resume state. Requests live in `io_req/<request>/` with deterministic fields that match `cepOpsAsyncIoReqInfo` (state DT, channel DT, opcode DT, beat budget, beat and wallclock deadlines, byte expectations/progress, errno, telemetry link).
- `cep_async_runtime_on_phase()` runs from the heartbeat’s Capture/Compute/Commit hooks and forwards the phase to `cep_io_reactor_on_phase()`. Capture stamps the beat number; Compute ingests completions from `cep_io_reactor_next_completion()`, rewrites the corresponding `io_req` entries, and only then lets `cep_ops_stage_commit()` deliver `op/cont` or `op/tmo` watcher signals; Commit simply leaves the queue drained. External subsystems (CPS, federation transports, serializer sinks) push completions through `cep_async_post_completion()` if they are delivering a result manually, but those completions still become visible exclusively during Compute so replay observes one deterministic ordering.
- `cep_flat_stream_emit_branch_async()` is the public serializer sink. Each branch controller hands its dirty root + `cepFlatBranchFrameInfo` to the helper so the buffered frame can be tagged with `{branch_domain,branch_tag,glob,frame_id}` before registering synthetic `begin`/`write`/`finish` requests on `cep_io_reactor`. The reactor chooses its backend via `CEP_IO_REACTOR_BACKEND=portable|epoll|auto` (defaulting to the bounded worker-thread shim) and can emit extra traces when `CEP_IO_REACTOR_TRACE` is set. Shim usage and timeouts emit `tp_async_unsp`, `persist.async`, or `persist.async.tmo` CEI topics and flip the stored `shim` bit so operators know when blocking fallbacks handled work. Metrics land in `/rt/analytics/async/reactor:io/{cq_depth,pend_bytes,comp_bt,timeouts,jobs_total}` so backlog and throughput stay observable.
- `cep_op_async_set_reactor_state()` records pause/shutdown metadata under `io_reactor/{draining,paused,shutdn,deadline_bt}` and `cep_io_reactor_quiesce()` enforces those flags with beat deadlines whenever Pause/Rollback/Resume or shutdown runs. Outstanding jobs are cancelled deterministically, and watcher TTLs (both under `/rt/ops/<oid>/watchers` and per-channel `watchers/`) expire into `op/tmo` entries when beats are exceeded, so clients see consistent cancellation semantics. `cep_async_reset_ops_oid()` tears down the default dossier during shutdown so new runtimes restart with a clean slate.

### Lifecycle operations, bootstrap, and packs
- `cep_l0_bootstrap()` initializes cells, the heartbeat, and namepool, then publishes `op/boot` states (`ist:kernel`, `ist:store`, `ist:packs`, `sts:ok`). `cep_heartbeat_emit_shutdown()` mirrors the process for `op/shdn` (`ist:stop`, `ist:flush`, `ist:halt`, `sts:ok`). `cep_op_await()` attaches beat-counted watchers that enqueue continuations via `op/cont` or `op/tmo` during stage commit.
- Upper-layer packs must bootstrap as optional plugins: probe prerequisites, register enzymes idempotently, publish readiness evidence through pack-owned ops or branches, and shut down cleanly. Kernel code must succeed with zero packs present.
- Organs describe typed subtrees with validator, constructor, and destructor enzymes. Descriptors live under `/sys/organs/<kind>/spec/` and are immutable. Validators must be bound with `propagate=true` and cannot be unbound. Constructors/destructors run via OPS dossiers so their progress is observable.

### Diagnostics, mailboxes, and CEI
- `/data/mailbox/diag` is seeded during bootstrap and acts as the default diagnostics mailbox. Mailboxes store messages under `msgs/<id>` plus metadata (`meta/policy`, `meta/runtime/expiries*`). Message IDs prefer caller input, fall back to envelope digests, then to counters to stay deterministic.
- TTL resolution merges message, mailbox, and topology scopes, computing beat and wallclock deadlines and recording the winning scope plus whether spacing analytics were involved. Retention planners read beat and wallclock buckets separately and report whether more work remains.
- CEI Facts contain severity (`sev:fatal|crit|usage|warn|debug`), topic, note, origin, optional subject link, beats/timestamps, and optional payload references. `cep_cei_emit()` can attach to OPS dossiers (closing them with `sts:fail` for `sev:crit` or `sev:fatal`) and optionally enqueue `sig_cei/<severity>` impulses. Fatal facts always trigger shutdown through the documented operation timeline.
- Severity mapping guidance: usage ⇒ misuse without shutdown, warn ⇒ tolerated anomaly, crit ⇒ integrity risk (auto shutdown), fatal ⇒ unrecoverable (immediate shutdown). Topics such as `control/prr`, `namepool.*`, and `transport/*` keep diagnostics searchable.

### Serialization, streaming, and replay
- Layer 0 emits a single flat frame per export via `cep_flat_serializer`. `cep_serialization_emit_cell` resolves the canonical cell, builds a `cepFlatFrameConfig` (beat number, apply mode, capability bitmap, hash/compression/checksum IDs, payload/manifest history windows), streams every record into the in-memory frame, and calls `cep_flat_serializer_finish` to append the trailer and optionally deflate the buffer before handing it to the sink. `cep_flat_serializer_frame_bytes` exposes the contiguous frame for transports that want to resend or persist it without re-serializing.
- Record taxonomy matches `cepFlatRecordType`: `cell_desc (0x01)` advertises metadata plus optional inline payloads (≤64 B), `payload_chunk (0x02)` streams blob bytes or proxy envelopes, `manifest_delta_pg (0x03)` and `order_delta_pg (0x04)` cover paged child sets and ordering hints, `namepool_delta (0x05)` teaches readers new DT IDs, `payload_history (0x06)`/`manifest_history_pg (0x07)` emit replay windows, and `frame_trailer (0xFF)` seals the frame. Each record stores its type/version/flags, varint-sized key/body lengths, the actual bytes, and a CRC32C footer. When payload fingerprints or paged child sets/order projections are present the serializer asserts `CEP_FLAT_CAP_PAYLOAD_FP`, `CEP_FLAT_CAP_PAGED_CHILDSET`, or `CEP_FLAT_CAP_PAGED_ORDER` respectively.
- The serializer hashes every record payload with BLAKE3, sorts the digests by key, and builds a Merkle tree whose root is written into the trailer alongside the beat, record count, apply mode, algorithm selectors, history selectors, and capability bitmap (the frame-compression bit is set whenever zlib runs). `cep_flat_serializer_add_caps` keeps the bitmap authoritative so readers can reject frames that advertise unsupported features before touching any record bytes.
- `payload_chunk` ordering is enforced end-to-end. Writers refuse to emit out-of-order offsets/ordinals, encode total size, chunk offset, chunk size, optional payload fingerprints, AEAD metadata (mode byte, nonce length, BLAKE3 AAD hash), and the payload bytes (ciphertext when AEAD is on). `cep_flat_reader` maintains per-base chunk trackers so it can detect duplicates, gaps, inconsistent totals, nonce mismatches, or zero-length chunks before the frame becomes visible.
- Optional features are gated by env selectors and mirrored in caps. `CEP_SERIALIZATION_FLAT_PAYLOAD_HISTORY_BEATS` and `CEP_SERIALIZATION_FLAT_MANIFEST_HISTORY_BEATS` control the historical record types (capped at 64 pages per store) and set `CEP_FLAT_CAP_{PAYLOAD,MANIFEST}_HISTORY` when anything was emitted. `cep_flat_namepool_emit` produces `namepool_delta` records and asserts `CEP_FLAT_CAP_NAMEPOOL_MAP` so readers learn DT references up-front. Store comparator metadata honours `CEP_SERIALIZATION_FLAT_MAX_COMPARATOR_VERSION`, aborting serialization the moment a comparator reports a newer version than the peer negotiated.
- AEAD/compression knobs live in the environment: `CEP_SERIALIZATION_FLAT_AEAD_MODE` (none, chacha20, xchacha20) plus `CEP_SERIALIZATION_FLAT_AEAD_KEY` enable libsodium-backed chunk encryption with deterministic nonces derived from the record key and payload fingerprint; unsupported AES-GCM requests are rejected with debug logs. `CEP_SERIALIZATION_FLAT_COMPRESSION=deflate` wraps the entire frame in a `CFLT` container recording raw/compressed sizes and the checksum algorithm before the zlib payload, and the serializer automatically asserts `CEP_FLAT_CAP_FRAME_COMPRESSION`.
- `cep_flat_reader_feed` ingests arbitrary byte slices, unwraps the optional `CFLT` container, verifies each record’s CRC32C, enforces chunk trackers, and `cep_flat_reader_commit` only flips `cep_flat_reader_ready` after the trailer parses cleanly and the recomputed Merkle root matches the advertised hash. Callers use `cep_flat_reader_frame`, `cep_flat_reader_records`, and `cep_flat_reader_merkle_root` to inspect configs, walk record arrays, or capture diagnostics; failures leave the reader in an error state so CEI can capture the offending topic.
- Production builds call `cep_flat_stream_emit_branch_async()` (via `cep_serialization_emit_branch_async()` wrappers inside CPS/federation helpers) instead of any legacy whole-`/data` helper. The async variant registers `begin`/`write`/`finish` OPS rows, submits the buffered branch frame to `cep_io_reactor`, and optionally mirrors bytes into a synchronous sink when `.require_sync_copy=true` in `cepFlatStreamAsyncStats`. Completion callbacks update the matching `io_req/<req>` entries; failures emit `persist.async` or `persist.async.tmo`, flip watchers into `op/tmo`, and abort the beat so CPS never advances `ist:store` on partial writes.
- Because every runtime client emits frames through `cep_flat_stream_emit_branch_async`, federation transports, archives, CAS writers, and replay tooling forward those branch-scoped frames as-is. HANDLE/STREAM payloads must keep using the flat stream helpers so proxy snapshots, AEAD settings, history selectors, and capability negotiation stay in sync during replay and episodic integration workflows.

### Persistence service (CPS)

#### Overview
CPS persists whichever `/data/<branch>` controllers flagged dirty work during the heartbeat: `cps_storage_commit_current_beat()` (invoked from `cep_heartbeat_stage_commit()` in `src/l0_kernel/cep_heartbeat.c`) walks the branch registry, consults each controller’s policy (durable, scheduled, on-demand, volatile), and hands the dirty root plus metadata to `cep_flat_stream_emit_branch_async()`. Each resulting frame lands in the active `cps_engine`, guaranteeing that observers only see fully committed beats at `N+1`. Branch directories mirror the in-memory subtrees (`branch.idx`, `branch.dat`, sparse checkpoints, frame directories, and `cas/` blobs) while `/data/persist/<branch>` exposes engine metrics, branch status (`branch_stat`), and the controller’s current policy (`config/`).

#### Technical details
- **Engine contract and capability negotiation.** `src/cps/cps_engine.h` defines the uniform vtable: engines must implement `begin_beat`, `put_record`, `commit_beat`, `abort_beat`, record lookups, prefix scans, checkpoint/compact/stats hooks, plus a `caps()` method that advertises `CPS_CAP_*` bits (beat atomicity, prefix scans, CRC32C, Merkle, AEAD, deflate, CAS dedup, history windows, namepool maps, remote backends). The storage service refuses to advance `ist:store` whenever the selected engine is missing or rejects the serializer’s capability bitmap.
- **Heartbeat hand-off.** During stage commit the kernel calls `cps_storage_commit_current_beat()`, which allocates a `cepFlatReader`, feeds it the beat’s frame, validates CRC32C/Merkle pairs via `cep_flat_reader_commit`, and finally invokes the engine’s transaction hooks. Any failure emits `persist.commit` CEI facts before the heartbeat reports the error, keeping OPS dossiers and telemetry aligned.
- **Async commit handshake.** `cps_storage_commit_current_beat()` registers the serializer’s `begin`/`write`/`finish` requests via `cps_storage_async_register_request()` (which internally calls `cep_async_register_request()` with opcodes such as `op:serial/begin` and `dt_cps_async_op_commit`). The storage service waits for completions by repeatedly calling `cep_async_runtime_on_phase(async_state, CEP_BEAT_COMPUTE)` inside `cps_storage_async_wait_for_commit()` until the matching `io_req/<req>` transitions to `ist:ok` or `ist:fail`. Timeouts mark the request `sts:cnl`, emit `persist.async.tmo`, and abort the beat; other failures emit `persist.async` info facts and keep the OPS evidence (state DT, byte counters, errno) in place for postmortem analysis.
- **Branch controller metrics and CEI.** Each controller tracks dirty-entry counts, byte estimates, pin counts, last beat, last frame ID, the most recent flush cause, and remembers how many bytes/pins the last flush drained. `cps_storage_commit_branch_requests()` publishes those stats into `/data/persist/<branch>/{metrics,branch_stat,config}` (fields such as `dirty_bytes`, `pin_count`, `flush_bytes`, `flush_pins`), then emits CEI topics while work runs: `persist.flush.begin`/`persist.flush.done`/`persist.flush.fail` wrap every frame, `persist.defer` records policy changes when an operator defers a branch, and the async helpers still emit `persist.async*` in failure cases. Operators call `op/br_flush`, `op/br_sched`, or `op/br_defer` to force an immediate flush, queue a future beat, or place the branch into on-demand mode; the verbs reuse the OPS timeline like the existing checkpoint/compact/sync/import tasks.
- **Cross-branch read decisions.** When `allow_volatile_reads` lets a durable or scheduled consumer read a dirty or volatile source, `cep_cell_svo_context_guard()` calls `cep_decision_cell_record_cross_branch()` so `/journal/decisions/<auto>` stores the beat, verb, consumer DT, source DT, and risk label before control returns to the enzyme; the guard still emits the `cell.cross_read` CEI fact. Replays call `cep_decision_cell_replay_begin()`/`cep_decision_cell_replay_end()` to consume that ledger in order, and the guard refuses the read if the recorded entry is missing or mismatched.
- **Policy scheduling knobs.** Scheduled-save controllers honour both `flush_scheduled_bt` (set by `op/br_sched`) and the recurring `flush_every_beats` interval (values `0`/`1` disable recurrence). The storage service treats explicit schedules as higher priority, then fires the periodic interval as soon as `current_beat - last_persisted_bt >= flush_every_beats`. Controllers tagged `flush_on_shutdown=true` automatically request a flush when shutdown reaches `ist:flush`, ensuring on-demand/scheduled branches persist before teardown completes.
- **Cross-branch read guard.** Low-level cell operations call `cep_branch_policy_check_read()` before cloning/updating data from another branch. When the source branch is volatile or still dirty, durable/scheduled consumers either fail (default) or, if their policy sets `allow_volatile_reads`, emit `cell.cross_read` CEI facts to record the decision before proceeding. This keeps OPS/CEI evidence in sync with policy-controlled cache reads.
- **Read-only snapshot policy.** `op/br_snapshot` seals a branch via `cep_branch_seal_immutable()`, flips `policy_mode="ro_snapshot"`, publishes `snapshot_ro=1` under `/config`, and emits `persist.snapshot` CEI so operators know the branch is immutable. Mutation attempts fail (the branch is sealed), and CPS treat the branch as read-only cache state; lazy-load hydration plus eviction metadata determine how long the snapshot sticks around in RAM.
- **Test bypass for CPS.** Kernel tests that do not exercise persistence call `test_runtime_enable_mock_cps()` (or export `CEP_TEST_MODE=mock_cps`) before `cep_l0_bootstrap()`. `cps_runtime_bootstrap()` then short-circuits, leaves `cps_runtime_is_ready()` false, and turns `cps_storage_commit_current_beat()` into a no-op so pause/resume and mailbox suites stop waiting on real branch flushes. Call `test_runtime_disable_mock_cps()` before suites that require the actual CPS backend.
- **Flatfile backend guarantees.** `cps_flatfile_commit_beat()` (`src/cps/cps_flatfile.c`) fsyncs staged payload (`branch.dat`) and index (`branch.idx`) segments, appends them in that order, writes the per-beat mini-TOC and trailer (record offsets, hashes, Merkle root), updates `branch.meta` plus the frame directory, and optionally snapshots checkpoints every `checkpoint_interval` beats. Success bumps `frames`, `beats`, `bytes_idx`, and `bytes_dat`, then calls `cps_flatfile_publish_metrics` so `/data/persist/<branch>/metrics` reflects the latest counters without touching the filesystem manually.
- **CAS cache and recovery flow.** The flatfile engine stores hashed blobs under `cas/XY/<hash>` plus `cas/manifest.bin`. `cps_flatfile_fetch_cas_blob_bytes` looks up payload references in that manifest first; misses fall back to the runtime `/cas` tree through `cps_flatfile_fetch_cas_blob_runtime`, emit `persist.recover` CEI events, and optionally cache the recovered blob. Every lookup refreshes `cas_hits`, `cas_miss`, and `cas_lat_ns`, exposing cache behaviour in `/data/persist/<branch>/metrics`.
- **Maintenance verbs and bundles.** CPS publishes `kv_eng="flatfile"` alongside metrics and processes maintenance OPS dossiers via `cps_storage_run_checkpoint`, `cps_storage_run_compact`, `cps_storage_run_sync`, `cps_storage_run_import`, and the branch-specific verbs (`cps_storage_run_branch_flush_op`, `cps_storage_run_branch_schedule_op`, `cps_storage_run_branch_defer_op`) in `src/cps/cps_storage_service.c`. `op/checkpt`/`op/compact` call the engine hooks; `op/sync` invokes `cps_storage_export_branch_bundle`, copying `branch.meta/.idx/.dat/.ckp/.frames` plus CAS blobs, writing a manifest, and verifying the bundle before announcing success; `op/import` re-verifies external bundles with `cps_storage_stage_bundle_dir`, promotes them atomically, and emits CEI summaries. Branch verbs manipulate controller policy (`force_flush`, `flush_scheduled_bt`, policy mode) so operators can force, schedule, or defer persistence without touching code. All verbs close their OPS dossiers with status notes and propagate severity (`persist.checkpoint`, `persist.flush.*`, `persist.frame.io`, `persist.bootstrap`, etc.) so operators can audit persistence activity.

#### Q&A
- **When does CPS run inside the beat?** After stream commits, OPS stage, and control progress have succeeded, `cep_heartbeat_stage_commit()` hands the frame to `cps_storage_commit_current_beat()`, so persistence always trails computation by one beat and never observes half-finished agendas.
- **How do we tell whether CAS is healthy?** Inspect `/data/persist/<branch>/metrics/{cas_hits,cas_miss,cas_lat_ns}` and watch `persist.recover` CEI entries; a rise in misses or recoveries means CPS fell back to the runtime `/cas` tree or could not locate the recorded blob.
- **What is the supported export/import flow?** Issue `op/sync` to create a manifest-signed bundle under `branch_dir/exports/` (containing the branch files plus CAS blobs) and rely on the CEI/OPS summary for auditing. On another node, run `op/import` so CPS re-verifies the bundle, stages it under `imports/`, and promotes it in one step; failures leave the existing branch untouched.

### Federation transports and organs
- Providers register once (`cep_fed_transport_register`), advertise capability bitmaps, and expose vtables for open/send/request_receive/close. The transport manager enumerates providers, mirrors metadata under `/net/transports/<id>/`, and seeds `/net/mounts`, `/net/peers`, and `/net/telemetry` via `configure_mount`.
- Mount schema stores `caps/required`, `caps/preferred`, `caps/upd_latest`, transport selections (`provider`, `prov_caps`, `upd_latest` flag), and telemetry counters (`ready_count`, `bp_count`, `fatal_count`, `last_mode`, `bp_flag`). Capability negotiation filters providers lacking required bits and respects preferred IDs when constraints are satisfied. `upd_latest` restricts mounts to unreliable transports that can drop stale payloads.
- Organs expose deterministic request dictionaries:
  - **Link**: `peer`, `mount`, `mode`, `local_node`, optional `pref_prov`, `allow_upd`, capability dictionaries. Validators write `state`, `provider`, and `error_note`, emit CEI topics (`tp_noprov`, `tp_schema`, etc.), and close mounts when requests are deleted.
  - **Mirror**: extends link schema with source peer/channel, bundle parameters (`beat_window`, `max_infl`, optional `commit_mode`, `resume_tok`), episodic integration (`bundle_seq`, `commit_beat`, `pend_resum`). Validators coordinate E3, enforce inflight limits, and publish CEI on timeouts or schema issues.
  - **Invoke**: routes remote enzyme submissions, enforces deadlines, and emits CEI on timeouts (`tp_inv_timeout`) or rejections (`tp_inv_reject`). Requests capture `peer`, `mount`, `local_node`, `pref_prov`, capability dictionaries, and optional beat deadlines.
- Telemetry and CEI mirror under `/net/peers/<peer>/ceh/<topic>` for diagnostics. Mock providers support deterministic tests, enabling dual-runtime harnesses without real transports.

### Developer workflow, integration practice, tuning, and roadmap
- Repository layout: `src/l0_kernel/cep_cell.*` (core), `src/l0_kernel/storage/*` (backends), `src/l0_kernel/cep_molecule.h` (utilities), `src/l0_kernel/stream/*` (adapters), `src/test/*` (munit suites). Follow deterministic coding conventions: assert preconditions, respect ownership, and prefer small inline helpers when safe.
- Integration steps: stage branches off-tree (`cep_cell_initialize_*`), populate them, attach via `cep_cell_add`/dictionary helpers, then finalize temporary shells. Never keep stale pointers across mutations; resolve paths each time.
- Testing guidance: rely on munit suites, re-run relevant tests after kernel changes, keep debug breadcrumbs under `meta/debug`, and remove them once investigations end. When running sanitizers, build separate ASAN and Valgrind variants; never stack Valgrind on ASAN.
- Performance tuning: choose the lightest payload/store combination, set enzyme registry capacity hints, size store buckets, avoid unnecessary reindexing, and prefer `_hard` updates when history is optional. Anti-patterns include repeated reindexing, using packed queues for sorted data, keeping enormous backlink sets, and recording history unnecessarily.
- Roadmap snapshot: bootstrap/identity and deterministic manipulation are complete; auto-ID hygiene, HANDLE/STREAM history, traversal telemetry, shadow cleanup, packed queue recycling, range queries, persistence hooks, and agency execution remain active focus/backlog items. Hybrid episode promotions/demotions already ship and need pack-level adoption. Upcoming milestones prioritize structural resilience, runtime telemetry, and extended stream helpers.
- Documentation hygiene: `DOCS-ORIENTATION-GUIDE.md` now includes the full document inventory; update it whenever docs move so the catalog stays reliable.

### Legacy integration parallels for PL/SQL developers
- Cells and stores align with table rows and nested tables; `/data/**` mirrors schema-owned storage, `/rt/**` functions like a redo buffer, and `/cas/**` replaces external BLOB stores.
- Beats replace transactions: all staged work becomes visible in the next beat and no observer can access partial state mid-beat.
- Enzymes replace stored procedures bound to schema objects; register descriptors once and bind them to target subtrees. Heartbeat `sig_sys/init` replaces “startup triggers”.
- Diagnostics and logging flow through mailboxes and CEI rather than ad-hoc tables; think of `cep_cei_emit` as raising an exception with deterministic routing.
- Long-running work models state via E3 episodes (with optional threaded executors) and pack-level schedulers instead of bespoke Rendezvous control planes.

## Q&A
- **How do I ensure a new helper aligns with existing invariants?** Re-read the relevant design note (for example, serialization or heartbeat), extend unit tests covering that subsystem, and only then modify code. Design docs capture the rationale that must remain true after refactors.
- **When should I use `_hard` updates or deletion helpers?** Only when history retention is unnecessary (e.g., cache resets) or when GC has proven no links remain. Regular workflows should prefer append-only updates so past states remain reconstructable.
- **What is the correct path for long-running orchestration?** Model it as an OPS/STATES dossier (`op/*` for generic tasks, `op/ep` for episodes). Awaiters, continuations, and shutdown/PRR controls all rely on that machinery, so bespoke polls should be avoided.
- **How do I route diagnostics outside the default mailbox?** Pass a pack-owned mailbox root into `cep_cei_emit` or custom mailbox helpers. TTL resolution, retention planning, and CEI severity policies remain identical regardless of the mailbox location.
