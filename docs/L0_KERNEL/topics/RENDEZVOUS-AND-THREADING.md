# Rendezvous, Pipelines, and Threads

## Introduction
Rendezvous turn CEP’s tidy heartbeat rhythm into a safe meeting point for long-running or parallel work. Flows can hand jobs to background threads or external workers, resume deterministically once results arrive, and keep every choice explainable.

## Technical Details
- **Ledger schema**: Entries live under `/data/rv/{key}` as dictionaries with normalised text fields. Beat counters (`spawn_beat`, `due`, `epoch_k`, `input_fp`, `deadline`, `grace_delta`, `max_grace`, `kill_wait`, `event_flag`, `grace_used`) store their values as text written by `rv_store_number`, keeping replay hashes consistent. Mandatory strings include `state` (initially `pending`), `on_miss` (`timeout`), `kill_mode` (`none`), `cas_hash` (empty string), and `signal_path` (derived automatically from the key when omitted). Optional strings cover `prof` (`rv-fixed` by default), `inst_id`, and callers may supply a telemetry subtree that lands under `telemetry/*` after first being cleared.
- **Off-tree staging**: `cep_rv_spawn()` builds new entries in a floating dictionary via `rv_build_entry`, while `cep_rv_report()` clones the existing entry with `rv_clone_entry`. Both paths stage updates away from `/data/rv`, invoke `cep_cell_finalize()` on failure, and only mutate the ledger by grafting the completed node with `rv_graft_entry`. That pattern keeps `/data/rv` append-only and lets callers roll back safely when helpers fail.
- **Helper APIs**: `cep_rv_prepare_spec()` merges defaults into a `cepRvSpec`, `cep_rv_spawn()` attaches the staged entry, `cep_rv_resched()` shifts `due`, `cep_rv_kill()` enforces kill policies, `cep_rv_report()` refreshes telemetry/state via a staged clone, and `cep_rv_capture_scan()`/`cep_rv_commit_apply()` bridge the heartbeat with the rendezvous queue. Register `rv_init`/`rv_route` by calling `cep_rendezvous_register()` so the runtime seeds defaults and mirrors completion signals into `/data/inbox/flow/inst_event`. `cep_rv_signal_for_key()` generates the flow signal path `CEP:sig_rv/<key>`.
- **Pipeline integration**: Flow transform steps spawn rendezvous jobs; Wait steps subscribe to the rendezvous signal path. The rv→flow bridge enzyme emits `/data/inbox/flow/inst_event/*` when `state` becomes `applied`, `timeout`, or `killed`, so the instance resumes on the next beat with the recorded telemetry. Kernel-maintained `event_flag` markers guarantee each completion emits exactly one event even if the state remains unchanged across later beats.
- **Threading profiles**: `rv-fixed` schedules a single due beat, `rv-epoch` repeats every *k* beats, `rv-cas` ingests `/cas` payloads, `observer` captures read-only telemetry, and `spec` explores speculative variants. Every profile stores its choice in the ledger so replays remain deterministic.
- **Mailroom updates**: `cep_mailroom_bootstrap()` now mirrors the namespaces described in `/sys/err_cat/**` instead of writing error codes itself. Flow and coherence bootstraps simply rely on the mailroom being ready.
- **Transactional follow-ups**: Rendezvous helpers still stage entries off-tree before grafting. A follow-up task (see `TODO.md`) evaluates whether the new `cep_mailroom_stage_request()` pattern and `cep_txn_*` could tighten ledger atomicity without losing append-only semantics.

## Q&A
- **How do I spawn a rendezvous?** Build a floating spec dictionary (or call `cep_rv_prepare_spec()` with your transform dictionary) and then `cep_rv_spawn(spec, key)`. The helper stages `/data/rv/{key}` off-tree, seeds defaults, and grafts the completed entry once every field is populated.
- **Why do helpers insist on off-tree staging?** Floating dictionaries let `cep_cell_finalize()` tear down partially written nodes when a helper fails, so `/data/rv` stays append-only. `rv_build_entry` and `rv_clone_entry` ensure every field is normalised before `rv_graft_entry` swaps the ledger child, avoiding half-initialised cells that previously triggered assertions.
- **How do I compute the rendezvous signal path when callers omit it?** Call `cep_rv_signal_for_key(&spec.key_dt, buffer, sizeof buffer)` after normalising the key. The helper emits `CEP:sig_rv/<tag>` so wait steps can subscribe deterministically.
- **Where do defaults live if the spec omits a field?** `cep_rv_prepare_spec()` merges `/defaults/<profile>` into the spec structure, supplying deterministic numbers, policies, and telemetry placeholders before the entry is staged.
- **Which helpers guarantee writable dictionaries?** Use `cep_cell_require_dictionary_store()` (wrapped inside `rv_ensure_dictionary_child`) to normalise ledger nodes, and rely on `cep_cell_copy_children(..., true)` to duplicate telemetry or child dictionaries when staging clones.
- **How do late jobs get supervised?** `cep_rv_capture_scan()`/`cep_rv_commit_apply()` evaluate `grace_delta`, `max_grace`, and `kill_mode`. States move through `pending → ready → applied` for on-time arrivals; otherwise they escalate to `late`, `timeout`, `killed`, or land in `quarantine`.
- **How do flows wake back up?** When `cep_rv_commit_apply()` marks `state=applied|timeout|killed`, the bridge enzyme writes an instance event under `/data/inbox/flow/inst_event`. Wait steps subscribed to `CEP:sig_rv/<key>` consume it next beat; `rv_emit_flow_event` mirrors telemetry, outcome, and optional signal or instance IDs.
- **What happens during restarts?** Rendezvous entries are ordinary cells. The rendezvous enzymes replay them deterministically as long as states and timestamps remain consistent. The staged-write pattern keeps the ledger consistent even if a crash happens mid-update.
